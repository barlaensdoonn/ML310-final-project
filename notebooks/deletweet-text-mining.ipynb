{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETWEET TEXT MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *INTRODUCTION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 67,756 tweets in this subset of the [Politwoops](https://projects.propublica.org/politwoops/) dataset, which is a collection of tweets deleted by US politician's while they were in office. The tweets in the dataset analyzed here were gathered from Nov. 17, 2011 - Feb 3, 2017. The database contains 11 fields:\n",
    "\n",
    "* id: unique id for the tweet [int]\n",
    "* user_name: twitter username, or author, of the tweet [str]\n",
    "* content: text content of the tweet [str]\n",
    "* created: date tweet was originally created [str; format ‘%m/%d/%Y %H:%M:S’]\n",
    "* modified: date tweet was last modified, in this case deleted [str; format ‘%m/%d/%Y %H:%M:S’]\n",
    "* tweet: the original tweet object from the Twitter Streaming API [json]\n",
    "* state: two letter code for politician's state [str]\n",
    "* party_id - number corresponding to politician's political party [int]\n",
    "  * 1 - Democrat\n",
    "  * 2 - Republican\n",
    "  * 3 - Independent\n",
    "  * 4 - Other\n",
    "* last_name - politician's last name [str]\n",
    "* first_name - politician's first name [str]\n",
    "* middle_name - politician's middle name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *PARSE AND TOKENIZE TEXT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any exploratory analysis can be done, the dataset must first be imported into a dataframe and preprocessed to remove any potentially broken rows that pandas.read_csv missed. From there a simple describe() shows us some preliminary information about the features in the dataset.\n",
    "\n",
    "For example we can see that each tweet's id is unique, which is good to know if we need to assign an identifier to any of the tweet's attributes. The twitter user TGforArkansas has deleted the most tweets in this timeframe, with 1,310 deletions. The content column shows us that not every tweet's text is unique, and in fact the most frequently deleted tweet has been posted and taken down 74 times. The state with the most deleted tweets by its representatives is California, which makes sense as it is one of the most populous states in the US, and as such has a proportionally high number of elected officials. The name fields give us the most common names for tweet deleters; Tim comes out on top here with 2,315 deletions, although of course this probably represents more than one Tim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1157: expected 11 fields, saw 141\\nSkipping line 2263: expected 11 fields, saw 77\\nSkipping line 2319: expected 11 fields, saw 92\\nSkipping line 4631: expected 11 fields, saw 129\\nSkipping line 8260: expected 11 fields, saw 89\\nSkipping line 8823: expected 11 fields, saw 84\\nSkipping line 8824: expected 11 fields, saw 129\\nSkipping line 10197: expected 11 fields, saw 131\\nSkipping line 10278: expected 11 fields, saw 123\\nSkipping line 10297: expected 11 fields, saw 123\\nSkipping line 10311: expected 11 fields, saw 123\\nSkipping line 10401: expected 11 fields, saw 79\\nSkipping line 10430: expected 11 fields, saw 154\\nSkipping line 10495: expected 11 fields, saw 92\\nSkipping line 12989: expected 11 fields, saw 77\\nSkipping line 14473: expected 11 fields, saw 73\\nSkipping line 16741: expected 11 fields, saw 79\\nSkipping line 22015: expected 11 fields, saw 81\\nSkipping line 22322: expected 11 fields, saw 123\\nSkipping line 22957: expected 11 fields, saw 74\\nSkipping line 24728: expected 11 fields, saw 81\\nSkipping line 28474: expected 11 fields, saw 73\\nSkipping line 29656: expected 11 fields, saw 75\\nSkipping line 29834: expected 11 fields, saw 100\\nSkipping line 31272: expected 11 fields, saw 88\\nSkipping line 37306: expected 11 fields, saw 76\\nSkipping line 42274: expected 11 fields, saw 76\\nSkipping line 46399: expected 11 fields, saw 75\\nSkipping line 47628: expected 11 fields, saw 103\\nSkipping line 47803: expected 11 fields, saw 74\\nSkipping line 53572: expected 11 fields, saw 77\\nSkipping line 54441: expected 11 fields, saw 72\\nSkipping line 55591: expected 11 fields, saw 68\\nSkipping line 61022: expected 11 fields, saw 72\\nSkipping line 61023: expected 11 fields, saw 72\\nSkipping line 64356: expected 11 fields, saw 76\\nSkipping line 65590: expected 11 fields, saw 73\\n'\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# import dataset and remove bad rows\n",
    "deletweet = pandas.read_csv('../data/deleted_tweets.csv', error_bad_lines=False)\n",
    "\n",
    "bad_rows = []\n",
    "\n",
    "for i in range(len(deletweet)):\n",
    "    if type(deletweet['tweet'][i]) != str:\n",
    "        bad_rows.append(i)\n",
    "    else:\n",
    "        tweet = json.loads(deletweet['tweet'][i])\n",
    "        if type(tweet) != dict:\n",
    "            bad_rows.append(i)\n",
    "\n",
    "deletweet.drop(deletweet.index[bad_rows], inplace=True)\n",
    "deletweet.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>content</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>tweet</th>\n",
       "      <th>state</th>\n",
       "      <th>party_id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67756</td>\n",
       "      <td>67756</td>\n",
       "      <td>67756</td>\n",
       "      <td>67756</td>\n",
       "      <td>67756</td>\n",
       "      <td>67756</td>\n",
       "      <td>67353</td>\n",
       "      <td>67756</td>\n",
       "      <td>67754</td>\n",
       "      <td>67754</td>\n",
       "      <td>5076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>67756</td>\n",
       "      <td>1647</td>\n",
       "      <td>67030</td>\n",
       "      <td>67475</td>\n",
       "      <td>61895</td>\n",
       "      <td>67756</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>948</td>\n",
       "      <td>465</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>390865980723040256</td>\n",
       "      <td>TGforArkansas</td>\n",
       "      <td>RT @derGeruhn: &amp;lt;script class=\"xss\"&amp;gt;$('.x...</td>\n",
       "      <td>05/26/2015 19:03:33</td>\n",
       "      <td>06/29/2012 17:40:36</td>\n",
       "      <td>{\"contributors\": null, \"truncated\": false, \"te...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>Griffin</td>\n",
       "      <td>Tim</td>\n",
       "      <td>Bernie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1310</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5854</td>\n",
       "      <td>32911</td>\n",
       "      <td>1667</td>\n",
       "      <td>2315</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id      user_name  \\\n",
       "count                67756          67756   \n",
       "unique               67756           1647   \n",
       "top     390865980723040256  TGforArkansas   \n",
       "freq                     1           1310   \n",
       "\n",
       "                                                  content  \\\n",
       "count                                               67756   \n",
       "unique                                              67030   \n",
       "top     RT @derGeruhn: &lt;script class=\"xss\"&gt;$('.x...   \n",
       "freq                                                   74   \n",
       "\n",
       "                    created             modified  \\\n",
       "count                 67756                67756   \n",
       "unique                67475                61895   \n",
       "top     05/26/2015 19:03:33  06/29/2012 17:40:36   \n",
       "freq                      5                   10   \n",
       "\n",
       "                                                    tweet  state party_id  \\\n",
       "count                                               67756  67353    67756   \n",
       "unique                                              67756     54        7   \n",
       "top     {\"contributors\": null, \"truncated\": false, \"te...     CA        2   \n",
       "freq                                                    1   5854    32911   \n",
       "\n",
       "       last_name first_name middle_name  \n",
       "count      67754      67754        5076  \n",
       "unique       948        465          36  \n",
       "top      Griffin        Tim      Bernie  \n",
       "freq        1667       2315        1048  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deletweet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct a list of strings to hold the tweet text\n",
    "tweet_text_raw = []\n",
    "\n",
    "for i in range(len(deletweet)):\n",
    "    tweet = json.loads(deletweet['tweet'][i])\n",
    "    tweet_text_raw.append(tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67756"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of individual tweets in the dataset\n",
    "len(tweet_text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct one long string of the dataset's tweet content\n",
    "tweet_string = ' '.join(tweet_text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the tweet tokenizer provided by NLTK\n",
    "# preserve_case=False will transform all to lowercase\n",
    "# strip_handles=True will remove all Twitter usernames from the text (i.e @justinbieber)\n",
    "# reduce_len=True will convert any repetition of a character more than 3 times to 3 repetitions (i.e. nooooo -> nooo)\n",
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "tweet_tokenized = tknzr.tokenize(tweet_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to NLTK text object for analysis\n",
    "text = nltk.Text(tweet_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102551"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make set of words for vocab\n",
    "words = [w.lower() for w in text]\n",
    "vocab = sorted(set(words))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08353323824783104"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical diversity\n",
    "len(vocab) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuation = ['.', ':', ',', '!', '\"', '-', '…', '...', \"’\", '?', '/', \"'\", '(', ')']\n",
    "filtered = [w for w in tweet_tokenized if w.lower() not in stopwords]\n",
    "filtered_again = [w for w in filtered if w.lower() not in punctuation]\n",
    "text_filtered = nltk.Text(filtered_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5811331574441604"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of text remaining after removing stopwords and punctuation\n",
    "len(text_filtered) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102388"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filtered = [w.lower() for w in text_filtered]\n",
    "vocab_filtered = sorted(set(words_filtered))\n",
    "len(vocab_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1435135218477289"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical diversity of processed text\n",
    "len(vocab_filtered) / len(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct frequency distributions for original processed texts\n",
    "fdist = nltk.FreqDist(text)\n",
    "fdist_filtered = nltk.FreqDist(text_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 50706),\n",
       " ('the', 36453),\n",
       " ('to', 35796),\n",
       " (':', 31383),\n",
       " (',', 22924),\n",
       " ('in', 19005),\n",
       " ('!', 17630),\n",
       " ('rt', 17591),\n",
       " ('for', 17011),\n",
       " ('of', 16543),\n",
       " ('a', 15379),\n",
       " ('and', 13102),\n",
       " ('on', 12671),\n",
       " ('at', 9034),\n",
       " ('&', 8852),\n",
       " ('i', 8481),\n",
       " ('is', 8119),\n",
       " ('you', 7652),\n",
       " ('\"', 7642),\n",
       " ('with', 7485),\n",
       " ('my', 6189),\n",
       " ('this', 6128),\n",
       " ('our', 5936),\n",
       " ('today', 5876),\n",
       " ('-', 5362),\n",
       " ('…', 5274),\n",
       " ('we', 5098),\n",
       " ('...', 4698),\n",
       " ('s', 4615),\n",
       " ('be', 4372),\n",
       " ('from', 4273),\n",
       " ('’', 4115),\n",
       " ('?', 4058),\n",
       " ('will', 4055),\n",
       " ('/', 4036),\n",
       " ('great', 3977),\n",
       " ('it', 3649),\n",
       " ('your', 3446),\n",
       " (\"'\", 3436),\n",
       " ('are', 3422),\n",
       " ('that', 3406),\n",
       " ('about', 3349),\n",
       " ('by', 3198),\n",
       " ('have', 3145),\n",
       " ('w', 3130),\n",
       " ('(', 2721),\n",
       " ('out', 2683),\n",
       " ('more', 2643),\n",
       " ('new', 2540),\n",
       " ('as', 2428)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common words in original text\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rt', 17591),\n",
       " ('&', 8852),\n",
       " ('today', 5876),\n",
       " ('great', 3977),\n",
       " ('w', 3130),\n",
       " ('new', 2540),\n",
       " ('day', 2288),\n",
       " ('thanks', 2286),\n",
       " ('us', 2262),\n",
       " ('support', 2241),\n",
       " ('$', 2198),\n",
       " ('house', 2056),\n",
       " ('thank', 1993),\n",
       " ('time', 1985),\n",
       " ('vote', 1954),\n",
       " ('help', 1733),\n",
       " ('join', 1695),\n",
       " ('bill', 1621),\n",
       " ('get', 1527),\n",
       " ('watch', 1484),\n",
       " ('congress', 1450),\n",
       " ('work', 1402),\n",
       " ('proud', 1402),\n",
       " ('need', 1399),\n",
       " ('live', 1332),\n",
       " ('morning', 1329),\n",
       " ('2', 1296),\n",
       " ('see', 1287),\n",
       " ('tonight', 1277),\n",
       " ('state', 1264),\n",
       " ('people', 1254),\n",
       " ('rep', 1250),\n",
       " ('president', 1246),\n",
       " ('act', 1241),\n",
       " (\"it's\", 1215),\n",
       " ('good', 1194),\n",
       " ('one', 1175),\n",
       " ('last', 1164),\n",
       " ('jobs', 1155),\n",
       " (\"i'm\", 1140),\n",
       " ('make', 1135),\n",
       " ('happy', 1117),\n",
       " ('#tg4lg', 1095),\n",
       " ('county', 1088),\n",
       " ('via', 1085),\n",
       " ('obama', 1076),\n",
       " ('first', 1060),\n",
       " ('senate', 1034),\n",
       " ('http', 1034),\n",
       " ('meeting', 1030)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common words in processed text\n",
    "# these are a vast improvement from above for determining the corpus's content\n",
    "fdist_filtered.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#tg4lg',\n",
       " 'bill',\n",
       " 'congress',\n",
       " 'county',\n",
       " 'first',\n",
       " 'good',\n",
       " 'great',\n",
       " 'happy',\n",
       " 'help',\n",
       " 'house',\n",
       " 'http',\n",
       " \"it's\",\n",
       " 'jobs',\n",
       " 'join',\n",
       " 'last',\n",
       " 'like',\n",
       " 'live',\n",
       " 'make',\n",
       " 'meeting',\n",
       " 'morning',\n",
       " 'need',\n",
       " 'obama',\n",
       " 'people',\n",
       " 'president',\n",
       " 'proud',\n",
       " 'senate',\n",
       " 'state',\n",
       " 'support',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'time',\n",
       " 'today',\n",
       " 'tonight',\n",
       " 'video',\n",
       " 'vote',\n",
       " 'watch',\n",
       " 'women',\n",
       " 'work']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words longer than 3 characters occurring more than 1000 times\n",
    "sorted(word for word in set(text_filtered) if len(word) > 3 and fdist_filtered[word] > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking forward; last night; town hall; health care; #tg4lg #jobsnow;\n",
      "make sure; high school; president obama; house floor; watch live;\n",
      "happy birthday; years ago; it's time; white house; good luck; supreme\n",
      "court; common sense; script class; middle class; great time\n"
     ]
    }
   ],
   "source": [
    "# words frequently appearing together in the text\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frequency distribition of the frequencies of word lengths\n",
    "dist_of_dist = nltk.FreqDist(len(w) for w in text)\n",
    "dist_of_dist_filtered = nltk.FreqDist(len(w) for w in text_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_of_dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_of_dist_filtered.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18264969246546497"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_of_dist.freq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15079095870979678"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_of_dist_filtered.freq(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
