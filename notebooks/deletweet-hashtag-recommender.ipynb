{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETWEET HASHTAG RECOMMENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *USE RESERVOIR SAMPLING TO SUBSET THE DATA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to cluster the data in a reasonable amount of time, a subset of the original dataset was needed. The [reservoir sampling algorithm](https://github.com/barlaensdoonn/reservoir_sampling/tree/master) implemented for this class was used to gather a truly random sample of 10,000 tweets.\n",
    "\n",
    "This section is just a demonstration and does not represent the actual subset used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dataset and extract tweets to feed algorithm\n",
    "deletweet = pandas.read_csv('../../deletweet/data/deleted_tweets_cleaned.csv')\n",
    "tweets = deletweet['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reservoir(stream, k):\n",
    "    '''\n",
    "    Populate sample space with first k items from stream. For remaining items in stream,\n",
    "    choose a random number j from 0 to item's index. If j is less than k, replace\n",
    "    jth element in sample with ith element from stream\n",
    "    '''\n",
    "    sample = stream[0:k]\n",
    "\n",
    "    for i in range(k, len(stream)):\n",
    "        j = randint(0, i + 1)\n",
    "        if j < k:\n",
    "            sample[j] = stream[i]\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take the sample\n",
    "samples = reservoir(tweets, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure we have 10,000 tweets\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Enjoyed another opportunity to visit with Nebraskans at today's Open Coffee in Holdrege. Thanks to all who attendend! http://t.co/9GTSUOQQTm\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at first sample\n",
    "tweet = json.loads(samples[0])\n",
    "tweet['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *EXAMINE SUBSET USED FOR CLUSTERING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "hashtag_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import subset used for clutering\n",
    "with open('../data/deletweet_subset_10000.json', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweets[tweet['id']] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets in subset\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract hashtags from tweets in subset\n",
    "for key in tweets.keys():\n",
    "    tweet = tweets[key]\n",
    "    if (tweet['entities']['hashtags']):\n",
    "        hashtag_dict[key] = [tweet['entities']['hashtags'][i]['text'] \\\n",
    "                             for i in range(len(tweet['entities']['hashtags']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4830"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many tweets in the sample have hashtags\n",
    "len(hashtag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7821"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = []\n",
    "for key in hashtag_dict.keys():\n",
    "    for tag in hashtag_dict[key]:\n",
    "        tags.append(tag)\n",
    "\n",
    "# how many individual hashtags are in the sample\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique hashtags are in the sample\n",
    "len(set(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *CLUSTER VIA JACCARD DISTANCE AND K-MEANS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the average number of unique hashtags in any subset of this dataset - which is roughly 40% of the number of tweets in the subset - 1,000 was the number of clusters chosen. This seemed to strike a balance between the uniqueness of the clusters and the likelihood that a cluster would have at least one hashtag in it that could be recommended to other tweets in the cluster.\n",
    "\n",
    "The clustering of 10,000 tweets into 1,000 clusters was done via K-Means clustering using Jaccard Distance as the distance metric. The implemenatation of the algorithm used is [open source](https://github.com/barlaensdoonn/Jaccard-K-Means), although I updated the code to make it run under Python 3.\n",
    "\n",
    "The clustering took over 50 hours to complete, so the code will not be included here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *IMPORT CLUSTERS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = pandas.read_csv('../data/clusters_1000_from_10000_reformat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_ids_int = []\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    nums = []\n",
    "    stripped = clusters['tweet_ids'][i].strip('{}').split(', ')\n",
    "\n",
    "    for num in stripped:\n",
    "        nums.append(int(num))\n",
    "    \n",
    "    tweet_ids_int.append(nums)\n",
    "    \n",
    "clusters['tweet_ids_int'] = tweet_ids_int\n",
    "    \n",
    "# same as above:\n",
    "# tweet_ids_int = [[int(num) for num in clusters['tweet_ids'][i].strip('{}').split(', ')] \\\n",
    "# for i in range(len(clusters))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse output of clustering algorithm into following format:\n",
    "#\n",
    "# {\n",
    "# cluster_01_id: {tweet_01_id: tweet_01_text}, [...], {tweet_n_id: tweet_n_text},\n",
    "# [...],\n",
    "# cluster_n_id: {tweet_01_id: tweet_01_text}, [...], {tweet_n_id: tweet_n_text}\n",
    "# }\n",
    "#\n",
    "# this takes about an hour, need to optimize\n",
    "clusters_dict = {}\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_dict[i] = {}\n",
    "    for j in range(len(deletweet)):\n",
    "        tweet_id = deletweet['id'][j]\n",
    "        if tweet_id in clusters['tweet_ids_int'][i]:\n",
    "            clusters_dict[i][tweet_id] = deletweet['content'][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *EXTRACT AND RECOMMEND HASHTAGS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clusters to use as demonstration\n",
    "demo_clusters = [3, 108, 150, 263, 374, 453, 509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 3\n",
      "309057990382714880: With @SenBlumenthal at today's Veterans of Foreign Wars hearing. #vets @DeptVetAffairs http://t.co/6atnheZMTe\n",
      "228553954458488832: As result of #HCR, more than 5.2M seniors &amp; people with disabilities have saved over $3.9B on Rx. http://t.co/BKAi4pWZ\n",
      "783109774753792002: A lot of people can’t handle it. —Trump on veterans with post-traumatic stress https://t.co/f05ggyEnb7\n",
      "169840713356423168: Gulf Coast Vietnam Veterans Salute http://t.co/wDbzAQUm\n",
      "174895471893028865: Met with Fred S. Sganga, Exec. Dir. of LI State Veterans Home. He briefed me on the State Veterans Homes Program. #NY9 http://t.co/utrNraqr\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(demo_clusters[0]))\n",
    "for key in clusters_dict[demo_clusters[0]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, tweettext=clusters_dict[demo_clusters[0]][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vets\n",
      "#HCR\n",
      "#SticksandStones\n",
      "#SAU\n",
      "#NY9\n"
     ]
    }
   ],
   "source": [
    "# extract hashtags from the tweets in the cluster\n",
    "cluster_tags = []\n",
    "\n",
    "for key in clusters_dict[demo_clusters[0]].keys():\n",
    "    for i in range(len(deletweet)):\n",
    "        if deletweet['id'][i] == key:\n",
    "            tweet = json.loads(deletweet['tweet'][i])\n",
    "            # pull out hashtags from tweet object\n",
    "            for tag in [tweet['entities']['hashtags'][i]['text'] \\\n",
    "                        for i in range(len(tweet['entities']['hashtags']))]:\n",
    "                if tag not in cluster_tags:\n",
    "                    cluster_tags.append(tag)\n",
    "                \n",
    "for item in cluster_tags:\n",
    "    print('#{}'.format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first example is typical of most of the clusters, and is a good indicator of the difficulties of trying to set up a hashtag recommendation system.\n",
    "\n",
    "On a general level the tweets in the above cluster have to do with veterans and health care. More specifically, 3 out of the 4 tweets mention veterans, while the outlier tweet is about health care and seniors, the latter of which happens to apply to many veterans. The cluster is a good condidate for a hashtag recommendation system, as the tweets have two overlapping, but separate themes from which to pull potential hashtags. And in fact, if our system were to recommend the hashtag #vets from the first tweet to the others in the cluster, it would be applicable to 2 out of the 3 remaining tweets. The next hashtag - #HCR, an acronym for \"health care reform\" - would be slightly less successful, in that it is applicable to only 1 out of the 3 remaining tweets.\n",
    "\n",
    "The last hashtag - #NY9, which stands for New York's 9th congressional district - presents a more complicated scenario, and one that appears frequently when attempting to recommend hashtags in this manner. While the tweet itself has been clustered correctly based on its text content, the hashtag is specific enough that the chances of it being applicable to another tweet in the dataset is very low.\n",
    "\n",
    "This situation arose many times when looking through the clusters; for example, tweets are correctly clustered together due to the fact that they all deal with sports, but the hashtags present in the cluster all reference specific teams, and therefore are not applicable to the other tweets in the cluster.\n",
    "\n",
    "This difficulty may be less of a problem as the size of the input dataset grows, as more data would presumably allow the clusters to become more specific. Another, potentially concurrent, method to alleviate this difficulty is to increase the number of clusters, also allowing for more specificity. However, both these potential solutions come at large computational costs, and the system is already prohibitvely costly in this domain. Increasing the number of clusters also lowers the number of hashtags available to each cluster, which is a disadvantage for recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 108\n",
      "746031782907129856: RT @RepKClark: 24 hours ago, we began this sit-in to demand votes on common sense gun safety measures. #NoBillNoBreak https://t.co/nTxqL35I…\n",
      "745821640970342401: RT @RepKClark: Staying on the House floor to demand a vote on gun safety bills. #NoBillNoBreak #NoMoreSilence #HoldTheFloor https://t.co/hv…\n",
      "745732604293320704: RT @WoodsGoods: Ty to @dinatitus who is holding her ground in support common sense gun violence prevention measures! #NoBillNoBreak\n",
      "745779309663518720: 9hrs and still on the House floor feeling united and proud to stand up as one to reduce gun violence #NoBillNoBreak https://t.co/AcSuS2cSCj\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(demo_clusters[1]))\n",
    "for key in clusters_dict[demo_clusters[1]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[demo_clusters[1]][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Yes2Wes\n",
      "#MascotMania\n",
      "#LittleLeague\n"
     ]
    }
   ],
   "source": [
    "# extract hashtags from the tweets in the cluster\n",
    "cluster_tags = []\n",
    "\n",
    "for key in clusters_dict[demo_clusters[1]].keys():\n",
    "    for i in range(len(deletweet)):\n",
    "        if deletweet['id'][i] == key:\n",
    "            tweet = json.loads(deletweet['tweet'][i])\n",
    "            # pull out hashtags from tweet object\n",
    "            for tag in [tweet['entities']['hashtags'][i]['text'] \\\n",
    "                        for i in range(len(tweet['entities']['hashtags']))]:\n",
    "                if tag not in cluster_tags:\n",
    "                    cluster_tags.append(tag)\n",
    "                \n",
    "for item in cluster_tags:\n",
    "    print('#{}'.format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 108 is a a successful example for the recommendation system. Each tweet has the hashtag #NoBillNoBreak, but one tweet in the cluster has 2 more that are highly applicable to the group, since all the tweets in the cluster deal with holding the floor to push for better gun safety policy: #NoMoreSilence and #HoldTheFloor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 263\n",
      "529816984637034496: RT @acberka: Just voted--now it's your turn! #republican #StandWithDan #goandvote http://t.co/OfTMLNO3sw\n",
      "523959270736674816: RT @BlueDevil83: @Hogan4Governor Your youngest fan!! HoganForMD http://t.co/BQJwjbNaZn\n",
      "407556287627399168: RT @smidgiekroger: @RepDennyHeck @RedCross check with your local American Legion Aux too\n",
      "523620595289030656: RT @KeevAdams3: Let's get t shirts printed #288MillionWentWhere @Hogan4Governor\n",
      "520017037645864960: RT @zapeters: Ran into Maryland's next Governor tonight - @Hogan4Governor #mdpolitics http://t.co/ISo1Gy7kOu\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(demo_clusters[3]))\n",
    "for key in clusters_dict[demo_clusters[3]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[demo_clusters[3]][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#republican\n",
      "#StandWithDan\n",
      "#goandvote\n",
      "#288MillionWentWhere\n",
      "#mdpolitics\n"
     ]
    }
   ],
   "source": [
    "# extract hashtags from the tweets in the cluster\n",
    "cluster_tags = []\n",
    "\n",
    "for key in clusters_dict[demo_clusters[3]].keys():\n",
    "    for i in range(len(deletweet)):\n",
    "        if deletweet['id'][i] == key:\n",
    "            tweet = json.loads(deletweet['tweet'][i])\n",
    "            # pull out hashtags from tweet object\n",
    "            for tag in [tweet['entities']['hashtags'][i]['text'] \\\n",
    "                        for i in range(len(tweet['entities']['hashtags']))]:\n",
    "                if tag not in cluster_tags:\n",
    "                    cluster_tags.append(tag)\n",
    "                \n",
    "for item in cluster_tags:\n",
    "    print('#{}'.format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 263 is a good example of a cluster with mixed success: #goandvote could apply to all the tweets; #mdpolitics and #288MillionWentWhere could apply to the 3 tweets that mention @HoganForGovernor; #StandWithDan is presumably not applicable to any of the other tweets in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 453\n",
      "266882923960094720: Update on #A #subway: Read the latest on rebuilding in the #Rockaways after Hurricane #sandy via @MTAInsider (pocs): http://t.co/UEoPWqjj\n",
      "269122784813273089: 'We Will Lead on #Climate Change' | Read the Gov's op-ed in @nydailynews: http://t.co/GYPkrbsg\n",
      "262232359192129536: Breaking: Gov's Dir of State Operations Howard Glazer, MTA Chairman Joseph Lhota, PANYNJ Exec Dir Pat Foye update on #Sandy 1:15PM NYC off\n",
      "265215852331278337: Stay Up To Date on Hurricane Sandy Recovery Efforts | #Sandy \n",
      "http://t.co/ATTnkJAg via @energy\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(demo_clusters[5]))\n",
    "for key in clusters_dict[demo_clusters[5]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[demo_clusters[5]][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#A\n",
      "#subway\n",
      "#Rockaways\n",
      "#sandy\n",
      "#Climate\n",
      "#Sandy\n"
     ]
    }
   ],
   "source": [
    "# extract hashtags from the tweets in the cluster\n",
    "cluster_tags = []\n",
    "\n",
    "for key in clusters_dict[demo_clusters[5]].keys():\n",
    "    for i in range(len(deletweet)):\n",
    "        if deletweet['id'][i] == key:\n",
    "            tweet = json.loads(deletweet['tweet'][i])\n",
    "            # pull out hashtags from tweet object\n",
    "            for tag in [tweet['entities']['hashtags'][i]['text'] \\\n",
    "                        for i in range(len(tweet['entities']['hashtags']))]:\n",
    "                if tag not in cluster_tags:\n",
    "                    cluster_tags.append(tag)\n",
    "                \n",
    "for item in cluster_tags:\n",
    "    print('#{}'.format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 453 is another example of a partially successful cluster: the first 2 hashtags are unlikely to be applicable to any of the other tweets, with the exception of potentially the 3rd tweet. However the hashtags #Climate and #Sandy have high likelihood of being applicable to all the tweets in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "While analyzing the clusters, an unforeseen situation arose in which a different kind of hashtag recommendation system might have some success. This is applicable to the clusters in which all the tweets have highly correlated content, but which have no existing hashtags. In this scenario, hashtags could be recommended based purely on the content of the clusters, most likely by turning a word common to all the tweets into a hashtag. This could be made more sophisticated and robust by searching in realtime to see if there are relevant popular hashtags on Twitter.\n",
    "\n",
    "We do not try to implement such a system here, but present some examples for potential future consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtags_from_content = [22, 31, 34, 146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 22\n",
      "630483076171309056: They oppose equal pay for equal work.\n",
      "522718529364443136: 1.3 million homeless students... http://t.co/yaMbgiTQkN\n",
      "519914410031067136: ...a pay raise for over 25 million American workers... http://t.co/M1E7WZne3R\n",
      "453607544230248448: RT @RepKevinBrady: All people deserve Equal Pay for Equal Work. Period. http://t.co/br1BG9d4Yr\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(hashtags_from_content[0]))\n",
    "for key in clusters_dict[hashtags_from_content[0]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[hashtags_from_content[0]][key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster 22: [#EqualPay4EqualWork](https://twitter.com/hashtag/EqualPay4EqualWork?src=hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 31\n",
      "702975909566029824: Thank you for https://t.co/qTtuzipdS2\n",
      "739253869050494977: @TeamTrumpNC thank you. https://t.co/YHF0wjEhhj\n",
      "703104748153499650: Thank you! Trump2016 #GOPDebate https://t.co/aV9rR1zl7o\n",
      "175614937446617088: @TheSmak Thank you!\n",
      "630050327371321345: Thank you #CruzCrew! #RSG15 #CruzCountry http://t.co/auapF8wQAi\n",
      "466422915215675392: @MJLeavitt thank you!\n",
      "461248994035773441: Thank you... http://t.co/v4ALozeg6T http://t.co/DkFdb8Wuj2\n",
      "375932319884128256: RT @jtylerharrison: @RepRickCrawford thank you\n",
      "266934031541743617: THANK YOU - http://t.co/qj7HEXBd\n",
      "466553301652090880: @Jennifer_K1691 thank you!\n",
      "203471882912137216: @DanVForbes Thank you. Appreciate the message\n",
      "312724010863575040: @USAFVeteran1 thank you!\n",
      "476559119466258432: @PolitiBunny Thank you!\n",
      "459093074636181505: This has been a blessing, thank you! http://t.co/dPVMqMCoPg\n",
      "423663010217881600: Thank you...\n",
      "161518376991207424: @Sloup Well, thank you 'Sloup'\n",
      "451419821976977408: Thank you sir “@daaman81: @NealMarchbanks daaman81@gmail.com”\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(hashtags_from_content[1]))\n",
    "for key in clusters_dict[hashtags_from_content[1]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[hashtags_from_content[1]][key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster 31: [#thankyou](https://twitter.com/hashtag/THANKYOU?src=hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 34\n",
      "808806195523911680: I liked a @YouTube video https://t.co/c681wo0UT6 Jay Z Gets Embarrassed By An Old Rapping Clip\n",
      "622157970084888576: Add a message to your video http://t.co/hpbd1mRVlI\n",
      "603271586091835394: I added a video to a @YouTube playlist http://t.co/pStiw1iZZX Obama Adminstration Failing to Enforce Immigration Law\n",
      "456614333561061380: I added a video to a @YouTube playlist http://t.co/ra93f6Zhgz Dr. Chad Mathis: Leader\n",
      "353243512831098881: I added a video to a @YouTube playlist http://t.co/omrRXEboOT Gov. Abercrombie Appoints Lt. Gov. Brian Schatz to U.S. Senate\n",
      "353243514148098048: I added a video to a @YouTube playlist http://t.co/lPrZ5Kb5p5 Senator Daniel K. Akaka Aloha Order of Merit Ceremony\n",
      "590987507032002561: Into'd a #RachelCarson res., thanks to activists like her bald eagles are back. Stellar @EPA video http://t.co/jvlfzYfcpV #EarthDay\n",
      "175302378776567808: I added a video to a @YouTube playlist http://t.co/igRQjoPI NBC Channel 11: Rep. Scott Tipton speaks about re\n",
      "353243514139717633: I added a video to a @YouTube playlist http://t.co/oZhzFRvNrY 6/19/12 News conference on Board of Education first year report as\n",
      "603275295848816641: I added a video to a @YouTube playlist http://t.co/tws34arkHF Rep. Collins: This Republican Majority is About Jobs\n",
      "487245128919052289: I added a video to a @YouTube playlist http://t.co/XfUd36pWQO McDermott on Immigration_PART II | MSNBC's Politics Nation I 7 9 14\n",
      "144891694213644289: I liked a @YouTube video from @larrymendte http://t.co/K4znTAkI Let Buddy Roemer Debate\n",
      "161828958894170114: I liked a @YouTube video http://t.co/DGz84Sw9 YouTube Town Hall: Where your view counts\n",
      "353241077098098688: I added a video to a @YouTube playlist http://t.co/y4lZfH6EYW Bill Signing for HB430 (Charitable Deducations)\n",
      "251817162522648576: I added a video to a @YouTube playlist http://t.co/KzOw6ocH Bannock Development Corporation Economic Symposium\n",
      "253186784421347328: I uploaded a @YouTube video http://t.co/olKeBAW4 Idaho Parks Passport Program\n",
      "400383072689860608: We owe our nation's #veterans a tremendous debt of gratitude. I was honored to spend Veterans Day with some of Maryland's heroes.\n",
      "603275278446665729: I added a video to a @YouTube playlist http://t.co/uP6AsrteaV Rep. Collins: High Education Needs Positive Solutions for American\n",
      "353241075823034369: I added a video to a @YouTube playlist http://t.co/FqzNRj5HiA Bill Signing SB856 (Civil Liberties &amp; Constitution Day)\n",
      "603275278484373504: I added a video to a @YouTube playlist http://t.co/5RQHYmnqPD Congressman Collins Talks About the U.S.--Israel Partnership\n",
      "353241075525222400: I added a video to a @YouTube playlist http://t.co/KaNd0RKuLl Kamuela Wassman Day\n",
      "603275278429851648: I added a video to a @YouTube playlist http://t.co/SwesdmyNCh Collins speaks in favor of the Executive Amnesty Prevention Act\n",
      "372746452612956161: I added a video to a @YouTube playlist http://t.co/w7A3KKPnD5 McNerney discusses the Sacramento-San Joaquin Delta, climate change,\n",
      "175302379128881152: I added a video to a @YouTube playlist http://t.co/h9r2xBBd Rep. Scott Tipton Q&A during Water and Power Subc\n",
      "603278040571977728: I added a video to a @YouTube playlist http://t.co/SaasQryIxz NSA: The New Four-Letter Word\n",
      "372746721270706176: I added a video to a @YouTube playlist http://t.co/0ly82irmM8 McNerney recognizing Army Staff Sergeant Ty Michael Carter\n",
      "157188236589006848: I uploaded a @YouTube video http://t.co/8yNbemAO Akin Update - KTRS Debate\n",
      "353241534134632448: I added a video to a @YouTube playlist http://t.co/pGdfkkfQjq Sequestration Press Conference3/1/13\n",
      "353241077144236032: I added a video to a @YouTube playlist http://t.co/uKLHbqvJCE Bill Signing, SB1077 (Relating to the Owner-Building Exemption)\n",
      "175302378780758016: I added a video to a @YouTube playlist http://t.co/14ruCXMs Rep. Scott Tipton Recaps 2011 Legislative Progres\n",
      "353241077140029441: I added a video to a @YouTube playlist http://t.co/BjOeOr9erD Bill Signings, SB682 (Relating to Fire Protection)\n",
      "372743657604276224: I added a video to a @YouTube playlist http://t.co/dPQbSdHLtw Debate on Amendment to protect Delta Agriculture\n",
      "175302378776563712: I added a video to a @YouTube playlist http://t.co/LPceXOSP Rep. Scott Tipton House Floor Speech 07-19-11\n",
      "262577631596249088: I uploaded a @YouTube video http://t.co/muzgbLUp Two Big Differences\n",
      "372746324112076800: I added a video to a @YouTube playlist http://t.co/85RDr8wiHg Congressman McNerney's speech on the importance of funding scientific\n",
      "372728439146823680: I added a video to a @YouTube playlist http://t.co/50146GjvHV Rep McNerney calls for fast and fair processing of short sales\n",
      "175302378747207681: I added a video to a @YouTube playlist http://t.co/2l3ITfzM Rep. Scott Tipton at Small Business Committee Sub\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(hashtags_from_content[2]))\n",
    "for key in clusters_dict[hashtags_from_content[2]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[hashtags_from_content[2]][key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster 34: [#youtube](https://twitter.com/hashtag/youtube?src=hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: 146\n",
      "410547295571046400: In case you missed it from The Wall Street Journal: How to Keep Workers Unemployed: http://t.co/2yGGtN6M3C\n",
      "368388131558785024: In case you missed it, see my guest appearance on PBS NewsHour discussing the suppressive tactics of NC's new... http://t.co/O3ERxEftHv\n",
      "335066013500579842: \"In case you missed it, here's clip from ABC's \"This Week\" talking about moms in Congress (and I got to take part)\" http://t.co/OKFI09pzIl\n",
      "142388919860867072: In case you missed it, we passed more #jobs bills out of @FinanceCmte yesterday http://t.co/hDTO7Wuj\n"
     ]
    }
   ],
   "source": [
    "# look at the tweets in a cluster\n",
    "print('cluster_id: {}'.format(hashtags_from_content[3]))\n",
    "for key in clusters_dict[hashtags_from_content[3]].keys():\n",
    "    print('{tweetid}: {tweettext}'.format(tweetid=key, \\\n",
    "                                          tweettext=clusters_dict[hashtags_from_content[3]][key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster 146: [#InCaseYouMissedIt](https://twitter.com/hashtag/InCaseYouMissedIt?src=hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## *CONCLUSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some proof of concept work has been done toward a hashtag recommendation system, there are significant opportunities for improvement.\n",
    "\n",
    "More specific text normalization has the potential to improve the robustness of the clusters. For example, links have very little information in this context, since they are generally shortened by twitter, and therefore any relevant text that may have existed in the original link is abstracted away.\n",
    "\n",
    "More importantly, the current system is prohibitively computationally expensive, and it seems that this approach would require hardware resources that are currently inaccessible to the avaerage consumer. Therefore this system may only be applicable in a theoretical or research context, rather than a production environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
