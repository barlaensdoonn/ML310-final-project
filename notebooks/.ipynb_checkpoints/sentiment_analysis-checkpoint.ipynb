{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import nltk.classify.util\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    '''\n",
    "    this function borrowed from: http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "\n",
    "    \"All of the NLTK classifiers work with featstructs, which can be simple dictionaries\n",
    "    mapping a feature name to a feature value. For text, we’ll use a simplified\n",
    "    bag of words model where every word is feature name with a value of True.\"\n",
    "    '''\n",
    "\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull out tokenized text from the classified tweets provided by NLTK\n",
    "# more info: http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus\n",
    "tokenized_negative = twitter_samples.tokenized('negative_tweets.json')\n",
    "tokenized_positive = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize text by transforming to lowercase\n",
    "negatives_normalized = [[word.lower() for word in thing] for thing in tokenized_negative]\n",
    "positives_normalized = [[word.lower() for word in thing] for thing in tokenized_positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run through wordfeats() to convert into featstructs for NLTK classifier\n",
    "negatives = [(word_feats(negatives_normalized[i]), 'neg') for i in range(len(tokenized_negative))]\n",
    "positives = [(word_feats(positives_normalized[i]), 'pos') for i in range(len(tokenized_positive))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 7500 instances, test on 2500 instances\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 75% train/25% test\n",
    "neg_split = int(len(negatives) * 0.75)\n",
    "pos_split = int(len(positives) * 0.75)\n",
    "train_feats = negatives[:neg_split] + positives[:pos_split]\n",
    "test_feats = negatives[neg_split:] + positives[pos_split:]\n",
    "print('train on {} instances, test on {} instances'.format(len(train_feats), len(test_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9936\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_feats)\n",
    "print('accuracy: {}'.format(nltk.classify.util.accuracy(classifier, test_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      :( = True              neg : pos    =   2214.3 : 1.0\n",
      "                      :) = True              pos : neg    =   1073.8 : 1.0\n",
      "                    glad = True              pos : neg    =     25.7 : 1.0\n",
      "                     x15 = True              neg : pos    =     23.7 : 1.0\n",
      "                 arrived = True              pos : neg    =     21.8 : 1.0\n",
      "                     sad = True              neg : pos    =     21.2 : 1.0\n",
      "                    sick = True              neg : pos    =     19.7 : 1.0\n",
      "               community = True              pos : neg    =     15.7 : 1.0\n",
      "                   loves = True              pos : neg    =     14.1 : 1.0\n",
      "                     ugh = True              neg : pos    =     13.7 : 1.0\n",
      "                    miss = True              neg : pos    =     13.3 : 1.0\n",
      "                      aw = True              neg : pos    =     13.0 : 1.0\n",
      "              definitely = True              pos : neg    =     13.0 : 1.0\n",
      "                   shame = True              neg : pos    =     12.3 : 1.0\n",
      "                follback = True              pos : neg    =     12.3 : 1.0\n",
      "                   didnt = True              neg : pos    =     12.3 : 1.0\n",
      "              appreciate = True              pos : neg    =     11.7 : 1.0\n",
      "                   hurts = True              neg : pos    =     11.0 : 1.0\n",
      "              bestfriend = True              pos : neg    =     11.0 : 1.0\n",
      "           @justinbieber = True              neg : pos    =     10.6 : 1.0\n",
      "                   sorry = True              neg : pos    =     10.2 : 1.0\n",
      "               followers = True              pos : neg    =     10.2 : 1.0\n",
      "                       ( = True              neg : pos    =     10.2 : 1.0\n",
      "                   tired = True              neg : pos    =     10.1 : 1.0\n",
      "                    huhu = True              neg : pos    =      9.7 : 1.0\n",
      "               goodnight = True              pos : neg    =      9.7 : 1.0\n",
      "                   enjoy = True              pos : neg    =      9.4 : 1.0\n",
      "                     via = True              pos : neg    =      9.3 : 1.0\n",
      "                   thank = True              pos : neg    =      9.1 : 1.0\n",
      "                 welcome = True              pos : neg    =      9.0 : 1.0\n",
      "                    cold = True              neg : pos    =      9.0 : 1.0\n",
      "           opportunities = True              pos : neg    =      9.0 : 1.0\n",
      "                   @uber = True              neg : pos    =      9.0 : 1.0\n",
      "           unfortunately = True              neg : pos    =      9.0 : 1.0\n",
      "                      :( = None              pos : neg    =      8.7 : 1.0\n",
      "                   great = True              pos : neg    =      8.4 : 1.0\n",
      "                  invite = True              pos : neg    =      8.3 : 1.0\n",
      "                     thx = True              pos : neg    =      8.3 : 1.0\n",
      "                 sharing = True              pos : neg    =      7.8 : 1.0\n",
      "                  missed = True              neg : pos    =      7.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFY DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1157: expected 11 fields, saw 141\\nSkipping line 2263: expected 11 fields, saw 77\\nSkipping line 2319: expected 11 fields, saw 92\\nSkipping line 4631: expected 11 fields, saw 129\\nSkipping line 8260: expected 11 fields, saw 89\\nSkipping line 8823: expected 11 fields, saw 84\\nSkipping line 8824: expected 11 fields, saw 129\\nSkipping line 10197: expected 11 fields, saw 131\\nSkipping line 10278: expected 11 fields, saw 123\\nSkipping line 10297: expected 11 fields, saw 123\\nSkipping line 10311: expected 11 fields, saw 123\\nSkipping line 10401: expected 11 fields, saw 79\\nSkipping line 10430: expected 11 fields, saw 154\\nSkipping line 10495: expected 11 fields, saw 92\\nSkipping line 12989: expected 11 fields, saw 77\\nSkipping line 14473: expected 11 fields, saw 73\\nSkipping line 16741: expected 11 fields, saw 79\\nSkipping line 22015: expected 11 fields, saw 81\\nSkipping line 22322: expected 11 fields, saw 123\\nSkipping line 22957: expected 11 fields, saw 74\\nSkipping line 24728: expected 11 fields, saw 81\\nSkipping line 28474: expected 11 fields, saw 73\\nSkipping line 29656: expected 11 fields, saw 75\\nSkipping line 29834: expected 11 fields, saw 100\\nSkipping line 31272: expected 11 fields, saw 88\\nSkipping line 37306: expected 11 fields, saw 76\\nSkipping line 42274: expected 11 fields, saw 76\\nSkipping line 46399: expected 11 fields, saw 75\\nSkipping line 47628: expected 11 fields, saw 103\\nSkipping line 47803: expected 11 fields, saw 74\\nSkipping line 53572: expected 11 fields, saw 77\\nSkipping line 54441: expected 11 fields, saw 72\\nSkipping line 55591: expected 11 fields, saw 68\\nSkipping line 61022: expected 11 fields, saw 72\\nSkipping line 61023: expected 11 fields, saw 72\\nSkipping line 64356: expected 11 fields, saw 76\\nSkipping line 65590: expected 11 fields, saw 73\\n'\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "deletweet = pandas.read_csv('../data/deleted_tweets.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct a list of strings to hold the tweet text\n",
    "tweet_text_raw = []\n",
    "\n",
    "for i in range(len(deletweet)):\n",
    "    if type(deletweet['tweet'][i]) == str:\n",
    "        tweet = json.loads(deletweet['tweet'][i])\n",
    "        if type(tweet) == dict:\n",
    "            tweet_text_raw.append(tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67756"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of individual tweets to classify\n",
    "len(tweet_text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the tweet tokenizer (casual.py) provided by NLTK\n",
    "# preserve_case = False will transform all to lowercase\n",
    "tknzr = TweetTokenizer(preserve_case=False)\n",
    "\n",
    "tokenized = [tknzr.tokenize(tweet_text_raw[i]) for i in range(len(tweet_text_raw))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run tokenized lists through word_feats() to construct featstruct for NLTK classifier\n",
    "features = [word_feats(tokenized[i]) for i in range(len(tokenized))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classed = classifier.classify_many(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_tweets = list(zip(classed, tweet_text_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg : This is so cool. This same sort of adaptive protocol is being used with shipping drones as well. https://t.co/pLuU2ljAmg\n",
      "pos : https://t.co/V7Rc07GrJU\n",
      "pos : #TBT @MikePenceVP https://t.co/tSZUjMjaaI\n",
      "pos : I had a cordial and candidate discussion today with the new DHS Secretary, John Kelly. https://t.co/4neFHS3Mji\n",
      "pos : Grt to host @USProgressives Specl Order w/@RepRaskin on #MuslimBan.Thx @RepMarkTakano @RepLawrence @RepGaramendi @RepCicilline @RepBarragan\n",
      "pos : I'm an original co-sponsor of @RepDonBeyer’s Freedom of Religon Act, protecting our values in response to @POTUS’ #MuslimBan. #FORAct\n",
      "neg : ⚡️ “Rieckhoff vs. Tester”\n",
      "\n",
      "https://t.co/JBr4v7j5PM\n",
      "pos : @IAVA CEO @PaulRieckhoff &amp; I are going #Head2Head to determine who dons the better ‘do. Post #Jon or #Paul to cast… https://t.co/GemjAHTZf7\n",
      "pos : @IAVA CEO @PaulRieckhoff &amp; I are going #Head2Head to determine who dons the better ‘do. Post #Jon or #Paul to cast your vote below!\n",
      "pos : @IAVA CEO @PaulReickhoff &amp; I are going #Head2Head to determine who dons the better ‘do. Reply with #Jon or #Paul to… https://t.co/D6JAMWIQVD\n",
      "pos : @IAVA CEO @PaulReickhoff and I are going #Head2Head to determine, once and for all, who dons the better ‘do. Cast y… https://t.co/IN8GU4Cf6i\n",
      "pos : @IAVA CEO @PaulReickhoff &amp; I are going #Head2Head to determine, once and for all, who dons the better ‘do. Cast you… https://t.co/XbDY4WicBO\n",
      "pos : .@HouseGOP have privately (&amp; rightfully) expressed fears about what #ACARepeal would mean for Americans. I thought… https://t.co/L29B3lPrAG\n",
      "pos : Not to worry. @realDonaldTrump promises to deliver a sensible, coherent plan for #MiddleEast peace on February 31. https://t.co/A3WYrOiIZY\n",
      "neg : Right now, Voting NO on going to Executive Session for nomination Price, Mnuchin and Sessions. No on Devos tomorrow morning.\n",
      "pos : These words take on new meaning in the #Trump Administration. https://t.co/TKHksDSGjn\n",
      "pos : .@HouseGOP have privately (&amp; rightfully) expressed fears about what #ACARepeal would mean for Americans. I thought… https://t.co/b9gwivlASJ\n",
      "pos : .@SenateMajLdr McConnell comments on measure --&gt; Video here: https://t.co/0yxGDq8cY6\n",
      "@WLKY https://t.co/WBq3aIm8Sc\n",
      "pos : .@SenateMajLdr McConnell comments on passage of anti-coal measure: https://t.co/K1ENddRMtH https://t.co/p9RkzurxDs\n",
      "pos : .@realDonaldTrump, FYI: Федеральная служба безопасности Российской Федерации =/= @CIA, @NSAGov, or @FBI.\n",
      "https://t.co/WkESVXnNA6\n",
      "neg : Tune into the now LIVE forum to hear from panelists, including Dr. Kahn about the #MuslimBan https://t.co/zqeyFkFppX https://t.co/suovuLy8Bg\n",
      "pos : JOBS: The AR1 will provide the US with a new, world-competitive engine for launch vehicles, 100 jobs for Alabamians. https://t.co/jRFqrUsNt8\n",
      "pos : We're working to ensure the hiring freeze does not prevent the @forestservice from preparing for wildfire season. https://t.co/RPOOEEzjsd\n",
      "pos : A new era of transparency begins at the @FCC  Thank you @AjitPaiFCC and @mikeoreilly https://t.co/ExPWdSKqhd\n",
      "pos : .@MichStatePolice still working hard to track down Officer Collin Rose’s killer, but they need your help… https://t.co/xHccCTuRS4\n",
      "pos : .@realDonaldTrump What's your stance on painkillers? Beta-endorphins invented at #UCBerkeley\n",
      "neg : Discrimination under the guise of \"religious freedom\" is still discrimination. I urge @POTUS not to reconsider.\n",
      "https://t.co/v2g9tOSzXP\n",
      "pos : @Fortunatebri I have no doubt. But I also know I can do a better job of jackasses currently in there - if you'll excuse my donkey mouth.\n",
      "pos : Great news from @AjiPaiFCC today – promise to make @FCC more open &amp; transparent, giving radio broadcasters more flexibility. #ProcessReform\n",
      "neg : RT @zenbeatnik: @Scotttaylorva But executed by Trump. The blaming of Obama must stop.\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print('{} : {}'.format(classified_tweets[i][0], classified_tweets[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
